{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow Housing SQL Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by Wolfrank Guzman\n",
    "@guzmanwolfrank:GitHub\n",
    "#\n",
    "email: guzmanwolfrank@gmail.com \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Objective:  Our objective is to use SQL queries to help analyze data from Zillow's Top Tier Housing Data.  Our goals are to see what changes have occured on Top Tier Home Values in all 50 states and Washington DC.  \n",
    "\n",
    "\n",
    "\n",
    "Zillow publishes top-tier ZHVI ($, typical value for homes within the 65th to 95th percentile range for a given region) and bottom-tier ZHVI.\n",
    "\n",
    "A user guide for this data can be found at: [Zillow](https://www.zillow.com/research/zhvi-user-guide/). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Load Data from CSV File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Wolfrank\\\\Desktop\\\\Zillow.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mguzmanwolfrank\\Data-SQL\\SQL Zillow\\ZillowProject.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a2264617461227d7d/guzmanwolfrank/Data-SQL/SQL%20Zillow/ZillowProject.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Load your data into a DataFrame (assuming it's in a CSV file)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a2264617461227d7d/guzmanwolfrank/Data-SQL/SQL%20Zillow/ZillowProject.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=14'>15</a>\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mWolfrank\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDesktop\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mZillow.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a2264617461227d7d/guzmanwolfrank/Data-SQL/SQL%20Zillow/ZillowProject.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=15'>16</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(csv_file)\n\u001b[0;32m     <a href='vscode-notebook-cell://github%2B7b2276223a312c22726566223a7b2274797065223a342c226964223a2264617461227d7d/guzmanwolfrank/Data-SQL/SQL%20Zillow/ZillowProject.ipynb#W4sdnNjb2RlLXZmcw%3D%3D?line=16'>17</a>\u001b[0m df \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\wolfr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wolfr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wolfr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\wolfr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\wolfr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\wolfr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1730\u001b[0m     f,\n\u001b[0;32m   1731\u001b[0m     mode,\n\u001b[0;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1738\u001b[0m )\n\u001b[0;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\wolfr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    858\u001b[0m             handle,\n\u001b[0;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    863\u001b[0m         )\n\u001b[0;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Wolfrank\\\\Desktop\\\\Zillow.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import csv \n",
    "import numpy as np \n",
    "import warnings \n",
    "from pandasql import sqldf\n",
    "# Ignore all warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Load your data into a DataFrame (assuming it's in a CSV file)\n",
    "\n",
    "csv_file = r\"C:\\Users\\Wolfrank\\Desktop\\Zillow.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "df = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing our libraries, let's clean the data by removing unnecessary columns.  Then, we add 1 to the Index and Size Rank columns.  We check the number of columns and display the resulting dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Next we clean up the data in the dataframe we just created, then we save the new file.\n",
    "\n",
    "# Data Cleaning:  Drop Columns \n",
    "columns_to_remove = ['RegionID', 'RegionType', 'StateName',]\n",
    "data.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# Rename Column from RegionName to State\n",
    "df.rename(columns={'RegionName': 'State'}, inplace=True)\n",
    "\n",
    "# Add 1 to Index \n",
    "df.index = df.index + 1\n",
    "\n",
    "# Add 1 to SizeRank \n",
    "df['SizeRank'] = df['SizeRank'] + 1\n",
    "\n",
    "# Show number of columns\n",
    "num_columns = len(data.columns)\n",
    "print(\"Number of columns:\", num_columns)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names and join them with a comma\n",
    "column_names = ','.join(df.columns)\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV data into a SQLite Database and connect to the database.  We can then write a list of SQL Queries for analysis and to generate results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV data into a SQLite database\n",
    "con = sqlite3.connect('zillow_data.db')\n",
    "\n",
    "df.to_sql('zillow_data', con, if_exists='replace', index=False)\n",
    "\n",
    "# Define SQL queries for analysis\n",
    "query1 = \"\"\"\n",
    "SELECT State, AVG(\"7/31/2019\") AS AvgHomeValue\n",
    "FROM zillow_data\n",
    "GROUP BY State\n",
    "\"\"\"\n",
    "\n",
    "query2 =  \"\"\"\n",
    "SELECT State, AVG(\"7/31/2023\") AS AvgHomeValue\n",
    "FROM zillow_data\n",
    "GROUP BY State\n",
    "\"\"\"\n",
    "\n",
    "query3 = \"\"\"\n",
    "SELECT State, AVG(\"7/31/2023\") AS AvgHomeValue\n",
    "FROM zillow_data\n",
    "GROUP BY State\n",
    "ORDER BY AvgHomeValue DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "query4 = \"\"\"\n",
    "SELECT State, (\"7/31/2023\" - \"7/31/2019\") AS ChangeInValue\n",
    "FROM zillow_data\n",
    "ORDER BY ChangeInValue DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "query5 =  \"\"\"\n",
    "SELECT AVG(\"7/31/2023\") AS average_value\n",
    "FROM zillow_data\n",
    "WHERE State = 'New York';\n",
    "\"\"\"\n",
    "query6 = \"\"\"\n",
    "SELECT AVG(\"7/31/2023\") AS average_value\n",
    "FROM zillow_data\n",
    "WHERE State = 'New Jersey';\n",
    "\"\"\"\n",
    "query7 = \"\"\" \n",
    "SELECT *\n",
    "FROM zillow_data\n",
    "WHERE (State = 'New York' OR State = 'New Jersey')\n",
    "  AND \"7/31/2018\" <= \"7/31/2023\";\n",
    "\"\"\"\n",
    "# Execute queries and fetch results\n",
    "query_results = []\n",
    "\n",
    "for query in [query1, query2, query3, query4, query5, query6, query7]:\n",
    "    result = pd.read_sql_query(query, con)\n",
    "    query_results.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn Visualizations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the output of the SQL Queries to construct visualizations such as histograms of the data we have cleaned and queried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Seaborn charts\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Chart 1: Average home values by state in September 2019\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=query_results[1], x='AvgHomeValue', y='State',palette=\"Greys_d\", alpha=0.8)\n",
    "plt.title('Average Home Values by State in July 2019')\n",
    "plt.xlabel('Average Home Value')\n",
    "plt.ylabel('State')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chart 2: Average home values by state in September 2021\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=query_results[0], x='AvgHomeValue', y='State',color=\"blue\", alpha=0.5)\n",
    "plt.title('Average Home Values by State in July 2023')\n",
    "plt.xlabel('Average Home Value')\n",
    "plt.ylabel('State')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 3:  Top 5 States with Highest Average Top Tier Home Values\n",
    "df = pd.read_sql_query(query3, con)\n",
    "# Create a Seaborn bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.barplot(data=df, x='AvgHomeValue', y='State', color=\"grey\", alpha=0.7)\n",
    "plt.title('Top 5 States with the Highest Average Home Values (as of 7/31/2023)')\n",
    "plt.xlabel('Average Home Value')\n",
    "plt.ylabel('State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the state with highest change in home value between July 2019 and July 2023 was Montana. Washington came in second whil Idaho, Florida and Utah rounded out the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Chart 4: Top 5  Highest Change in Top Tier Home Value \n",
    "result4 = pd.read_sql_query(query4, con)\n",
    "# Create Seaborn histogram chart\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=result4, x='ChangeInValue', y='State', color=\"blue\", alpha=0.6)\n",
    "plt.title('Top 5 States with Highest Change in Home Values (Jul 2019 to Jul 2023)')\n",
    "plt.xlabel('Change in Home Value')\n",
    "plt.ylabel('State')\n",
    "# Show the Charts\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By overlaying the data from 2019 and 2023 we can see the change in price frequency at certain price points.  \n",
    "\n",
    "This stacked count histogram shows us that there was a higher amount of mid priced homes at 200 to 400k in 2019 and that inventory has shrunken.  It has been replaced by higher priced homes amongst a smaller number of states-- as shown by the lower frequency counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chart 5: Top Tier Home Value Frequency for 2019 and 2023\n",
    "df1 = pd.read_sql_query(query1, con)\n",
    "df2 = pd.read_sql_query(query2, con)\n",
    "# Create stacked histograms using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df1['AvgHomeValue'], bins=20, color='grey', alpha=0.7, label='Home Avg 2019')\n",
    "sns.histplot(df2['AvgHomeValue'], bins=20, color='blue', alpha=0.5, label='Home Avg 2023')\n",
    "plt.title('Frequency of Top Tier Home Values for 2019 & 2023')\n",
    "plt.xlabel('Average Top Tier Home Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two line chart below, we can visualize the average values in 2019 and 2023.  These values are identified by their corresponding State in the X-axis.  We can also see the sharp discrepancy between different states in terms of average value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 6:  Average Top Tier Home Value\n",
    "df1 = pd.read_sql_query(query1, con)\n",
    "df2 = pd.read_sql_query(query2, con)\n",
    "\n",
    "# Create a two-line chart using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df1, x='State', y='AvgHomeValue', label='Home Avg 2019', color='blue')\n",
    "sns.lineplot(data=df2, x='State', y='AvgHomeValue', label='Home Avg 2023', color='grey')\n",
    "\n",
    "plt.title('Avg Home Value by State (2019 vs. 2023)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('AvgHomeValue')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# Rotate the X-axis text by 90 degrees\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can take a look at specific locations.  In this instance we will Query New York and New Jersey and note any discrepancies in value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the SQL query to calculate the average\n",
    "query5 = \"\"\"\n",
    "SELECT AVG(\"7/31/2023\") AS average_value\n",
    "FROM zillow_data\n",
    "WHERE State = 'New York';\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "cursor = con.execute(query5)\n",
    "result = cursor.fetchone()\n",
    "\n",
    "\n",
    "# Print the average value\n",
    "average_value = result[0]\n",
    "print(f\"The average home value for New York Top Tier Homes according to Zillow on 7/31/2023 is: {average_value*1000}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the SQL query to calculate the average\n",
    "query6 = \"\"\"\n",
    "SELECT AVG(\"7/31/2023\") AS average_value\n",
    "FROM zillow_data\n",
    "WHERE State = 'New Jersey';\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "cursor = con.execute(query6)\n",
    "result = cursor.fetchone()\n",
    "\n",
    "# Close the database connection\n",
    "#con.close()\n",
    "\n",
    "# Print the average value\n",
    "average_value = result[0]\n",
    "print(f\"The average home value for New Jersey Top Tier Homes according to Zillow on 7/31/2023 is: {average_value*1000}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see a SQL Query that is filtered using Pandas that shows Zillow Top Tier Home Values for the last 4 years for New York and New Jersey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query7 = \"\"\" \n",
    "SELECT *\n",
    "FROM zillow_data\n",
    "WHERE (State = 'New York' OR State = 'New Jersey')\n",
    "  AND \"7/31/2018\" <= \"7/31/2023\";\n",
    "\"\"\"\n",
    "filtered_df = pd.read_sql_query(query7, con)\n",
    "\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas DataFrame shows us that home values for top tier homes in New York and New Jersey are steadily rising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at a boxplot that shows the standard deviation of the index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_sql_query(query3, con)\n",
    "\n",
    "# Create a boxplot using Seaborn\n",
    "sns.set(style=\"whitegrid\")  # Optional: Set the style for the plot\n",
    "\n",
    "# Replace 'column_name' with the name of the column you want to create a boxplot for\n",
    "sns.boxplot(data=df, x='Average Home Value')\n",
    "\n",
    "# Optional: Customize the plot further (e.g., add labels, title, etc.)\n",
    "plt.xlabel(\"X-axis Label\")\n",
    "plt.ylabel(\"Y-axis Label\")\n",
    "plt.title(\"Boxplot of DataFrame Column\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran 7 SQL Queries using SQLite and Python to realize that home values, in Zillow's Top Tier Home Value Data have risen dramatically.  \n",
    "\n",
    "We also learned on Chart 5, a histogram, that the amount of mid range priced homes is dwindling.  Homes across the Top Tier are becoming more expensive and concentrating in a handful of states.  \n",
    "\n",
    "Chart 4 also taught us some new developments, surprisingly to some, Montana has had the highest overall change in Top Tier Home Value across the United States dating back to 2019. \n",
    "\n",
    "Overall, using SQLite, SQL Queries and Python we were able to analyze Zillow Top Tier Home Data for emerging trends and specific locations with higher values.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
